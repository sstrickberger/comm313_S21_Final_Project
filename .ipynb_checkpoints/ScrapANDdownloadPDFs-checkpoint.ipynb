{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's pull in our data -- writings from Black abolitionists "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from string import ascii_uppercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create base URLS for later in the process\n",
    "\n",
    "base_url = 'https://libraries.udmercy.edu/find/special_collections/digital/baa/'\n",
    "base_url_letter = 'https://libraries.udmercy.edu/find/special_collections/digital/baa/index.php?browseBy=DC_creator&letter={}&cloud=n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how the {} will work...here's an example:\n",
    "#text = 'test{}'\n",
    "#text.format('love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to sift through each letter for author URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_author_URL(letter_URL):\n",
    "    \"\"\"\n",
    "    \n",
    "    This function sifts through every letter URL -- \n",
    "    e.g. the webpage with all A authors -- \n",
    "    to pull out the document page URL.\n",
    "    \n",
    "    ARGUMENT: letter_URL is the URL for each letter webpage\n",
    "    VALUES: URLS for each author's homepage listed on the initial link\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    locate_letter_page = requests.get(letter_URL)\n",
    "    letter_page_text = BeautifulSoup(locate_letter_page.text, 'html.parser')\n",
    "    author_codes = letter_page_text.find_all('p')\n",
    "\n",
    "    author_links = []\n",
    "\n",
    "    for author_code in author_codes:\n",
    "        author_link = base_url + author_code.find('a').attrs['href'] #find all author links\n",
    "        author_links.append(author_link)\n",
    "\n",
    "    \n",
    "    return author_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to sift through all document of a given author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_documents(author_URL):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    This function helps sift through all the documents\n",
    "    of a given author's page.(Most have one, but some have multiple.)\n",
    "    At the end, it spits out each specific document page links.\n",
    "    \n",
    "    ARGUMENT: author_URL is the webpage on which all documents are listed that they wrote\n",
    "    VALUES: URLS for each document written by the author of the URL\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    locate_author_documents_page = requests.get(author_URL)\n",
    "    author_documents_page = BeautifulSoup(locate_author_documents_page.text, 'html.parser')\n",
    "\n",
    "    all_links = author_documents_page.find_all('a', class_='listLink') #find each link's general location\n",
    "\n",
    "    document_links = [] #create new list to hold all the document links\n",
    "    for link in all_links:\n",
    "        document_link = base_url + link['href'] #paste the base link together with the cleaned up found link\n",
    "        document_links.append(document_link)\n",
    "    \n",
    "    return document_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST?\n",
    "\n",
    "#url = \"https://libraries.udmercy.edu/find/special_collections/digital/baa/index.php?collectionCode=baa&field=DC_creator&term=%22Wake%2C+Ransom+F.%22\"\n",
    "url = 'https://libraries.udmercy.edu/find/special_collections/digital/baa/index.php?collectionCode=baa&field=DC_creator&term=%22White%2C+Jacob+C.%2C+d.+1872%22'\n",
    "documents_URL = find_all_documents(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pull out each PDF and download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_PDF_URL(document_URL, output_folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function provides the final main step of my crawling. \n",
    "    It will find the PDF links for URL's given document.\n",
    "    \n",
    "    ARGUMENT: document_URL is the page that houses the PDF links\n",
    "    VALUES: Downlaod  each PDF document.\n",
    "    \"\"\"\n",
    "    \n",
    "    #LOADING IN THE page with the PDF\n",
    "    locate_document_page = requests.get(document_URL)\n",
    "    document_page = BeautifulSoup(locate_document_page.text, 'html.parser')\n",
    "\n",
    "    document_page.text.find('Click to view PDF')\n",
    "\n",
    "    if document_page.text.find('Click to view PDF') == -1:\n",
    "        print('not a PDF')\n",
    "        return None\n",
    "\n",
    "    #Search for bolded (aka important) notes regarding the data\n",
    "    key_terms_dict = {}\n",
    "    keys = []\n",
    "    values = []\n",
    "    for p in document_page.find_all('p'):\n",
    "\n",
    "        try: \n",
    "            if len(p.find('strong')):\n",
    "\n",
    "                keys.append(p.text.split(\": \")[0])\n",
    "                values.append(p.text.split(\": \")[1])\n",
    "\n",
    "        except:\n",
    "            print(\"No 'strong' in this line\")\n",
    "\n",
    "    for i in range(len(keys)):\n",
    "        key_terms_dict[keys[i]] = values[i]\n",
    "\n",
    "    \n",
    "    # Find the PDF Link \n",
    "    PDF_URL = document_page.find('td').find('a')['href']\n",
    "    \n",
    "    # NEED HELP!!\n",
    "    # Download PDF LINK -- NEED HELP\n",
    "    PDF = requests.get(PDF_URL)\n",
    "\n",
    "    # name file by Title (author) and date published\n",
    "    filename = f'{key_terms_dict[\"Title\"]}_{key_terms_dict[\"Date published\"]}'\n",
    "\n",
    "    # in case a file already exists, add digits at the end [TAKEN FROM JAKE DIRECTLY]\n",
    "    if os.path.isfile(os.path.join(output_folder,filename+'.pdf')):\n",
    "        filename += \"_\" + str(random.randint(100,999))\n",
    "\n",
    "    # Create the file name in the dictionary [TAKEN FROM JAKE DIRECTLY]\n",
    "    key_terms_dict['filename'] = filename + '.pdf'\n",
    "\n",
    "    # save PDF [TAKEN FROM JAKE DIRECTLY]\n",
    "    print(f'saving -- {filename}.pdf')\n",
    "    PDF = requests.get(PDF_URL)\n",
    "    with open(os.path.join(output_folder,filename+'.pdf'),'wb') as pdf:\n",
    "        pdf.write(PDF.content)\n",
    "    return key_terms_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch for the final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://libraries.udmercy.edu/find/special_collections/digital/baa/item.php?record_id=2605&collectionCode=baa'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_URL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOADING IN THE page with the PDF\n",
    "\n",
    "document_URL = documents_URL[2]\n",
    "\n",
    "locate_document_page = requests.get(document_URL)\n",
    "document_page = BeautifulSoup(locate_document_page.text, 'html.parser')\n",
    "\n",
    "document_page.text.find('Click to view PDF')\n",
    "\n",
    "if document_page.text.find('Click to view PDF') == -1:\n",
    "    print('not a PDF')\n",
    "    #return none\n",
    "\n",
    "\n",
    "#pick out key info regarding the data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'strong' in this line\n",
      "No 'strong' in this line\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Date published': '1859-08',\n",
       " 'Description of file(s)': 'PDF 4 page, 854 word document (text and images)',\n",
       " 'Keywords': 'August 1st; British; celebration; emancipation; England; freedom; Jamaica; West Indies',\n",
       " 'Newspaper or publication': 'Weekly Anglo-African (1859 - 1862)',\n",
       " 'People': 'Bleby, Rev. Henry; Clarkson, Thomas; Wilberforce, William',\n",
       " 'Publication type': 'Newspapers; Speeches',\n",
       " 'Speaker or author': 'White, Jacob C., d. 1872',\n",
       " 'Subjects': 'Abolitionists--United States; African American abolitionists; Antislavery movements--United States; Slavery; United States--History--19th century',\n",
       " 'Title': 'Jacob C. White, Jr.'}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Search for bolded (aka important) notes regarding the data\n",
    "key_terms_dict = {}\n",
    "keys = []\n",
    "values = []\n",
    "for p in document_page.find_all('p'):\n",
    "    \n",
    "\n",
    "    try: \n",
    "        if len(p.find('strong')):\n",
    "\n",
    "            keys.append(p.text.split(\": \")[0])\n",
    "            values.append(p.text.split(\": \")[1])\n",
    "               \n",
    "    except:\n",
    "        print(\"No 'strong' in this line\")\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    key_terms_dict[keys[i]] = values[i]\n",
    "        \n",
    "key_terms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving -- Jacob C. White, Jr._1859-08_587.pdf\n"
     ]
    }
   ],
   "source": [
    "# NEED HELP!!\n",
    "# Download PDF LINK -- NEED HELP\n",
    "PDF = requests.get(PDF_URL)\n",
    "\n",
    "# name file by Title (author) and date published\n",
    "filename = f'{key_terms_dict[\"Title\"]}_{key_terms_dict[\"Date published\"]}'\n",
    "\n",
    "# in case a file already exists, add digits at the end [TAKEN FROM JAKE DIRECTLY]\n",
    "if os.path.isfile(os.path.join('sam_test',filename+'.pdf')):\n",
    "    filename += \"_\" + str(random.randint(100,999))\n",
    "\n",
    "# Find the PDF Link \n",
    "PDF_URL = document_page.find('td').find('a')['href']\n",
    "\n",
    "# Create the file name in the dictionary\n",
    "key_terms_dict['filename'] = filename + '.pdf'\n",
    "\n",
    "\n",
    "# save PDF\n",
    "print(f'saving -- {filename}.pdf')\n",
    "PDF = requests.get(PDF_URL)\n",
    "with open(os.path.join('sam_test',filename+'.pdf'),'wb') as pdf:\n",
    "    pdf.write(PDF.content)\n",
    "#return key_terms_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's bring the pieces together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ascii_uppercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document_json = []\n",
    "\n",
    "#sift through every letter page for the authors on it\n",
    "for letter in ascii_uppercase[:1]:  #just do A for now\n",
    "    letter_URL = base_url_letter.format(letter) #find the URL for the first letter, etc\n",
    "    \n",
    "    authors_URL = find_author_URL(letter_URL) #find the URL for each author on the letter page \n",
    "\n",
    "    # sift through the authors page for all of their documents\n",
    "    for author_URL in authors_URL:\n",
    "        documents_URL = find_all_documents(author_URL)\n",
    "        \n",
    "        # find docu\n",
    "        for document_URL in documents_URL:\n",
    "            download_pdf = final_PDF_URL(document_URL, 'sam_test/') #download and get pdf info\n",
    "            document_json.append(download_pdf) # save doc info to json        \n",
    "        \n",
    "#find_all_documents(author_URL), \n",
    "    #final_PDF_URL(document_URL, output_folder)\n",
    "    \n",
    "\n",
    "\n",
    "#find_author_URL(letter_URL), \n",
    "#find_all_documents(author_URL), \n",
    "#final_PDF_URL(document_URL, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
